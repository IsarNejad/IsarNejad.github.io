---
permalink: /
title: " "
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Senior Research Scientist at the [National Research Council Canada](https://nrc.canada.ca/en) and an Adjunct Professor at the [University of Ottawa](https://www.uottawa.ca/faculty-engineering/school-electrical-engineering-computer-science). My research focuses on leveraging AI-enabled technologies to address societal challenges, with a strong commitment to integrating ethical considerations into technological advancements.

I am currently working with the Canadian AI Safety Institute (CAISI) on several projects related to the evaluation and mitigation of safety risks in AI applications. I am also part of the research group that represents Canada in the Joint Testing of LLMs and agentic AI, conducted by the network of international AI Safety Institutes.

Before joining NRC, I was the Head of AI Research at IMRSV Data Labs, where I led a team of researchers and engineers in delivering AI solutions for Canadian businesses.

I am an active member of the Association for Computational Linguistics (ACL) community and serve as an area chair at leading NLP conferences such as ACL, EMNLP, and NAACL. I am also a committee member for several ACL workshops on Ethical NLP, including WOAH (Workshop on Online Abuse and Harms), TrustNLP (Trustworthy Natural Language Processing), and PrivateNLP (Privacy in Natural Language Processing).

Outside of work, I enjoy cooking, gardening, and weight training.

# Latest News

{: .amaranth-deep-purple}
### January 2026 - AI Impact Summit

I will be attending the AI Impact Summit in India February 16th -20th. 

{: .amaranth-deep-purple}
### December 2025 - Panelist

I was a panelsit at the Migration Policy-Research Forum organized by IRCC and the Toronto Metropolitan University. This forum brought together leading researchers and experts from around the world to inform Canada's migration policy. I spoke about the sociotechnical evaluation of AI once deployed in a critical domain such as migration. The discussions in this forum were timely, relevant, and enriched by global perspectives. 

{: .amaranth-deep-purple}
### October 2025 - Invited Talk 

I presented to the participants of the **Executive Leadership Development Program** at the **Canada School of Public Service**. The session was centred around **‚ÄúAI, Power, and Protocol: Building Ethical Systems for Public Good‚Äù**. I spoke about how biased and flawed technology can exacerbate the already existing inequalities. This was an incredibly interactive session, and we discussed how all of us, especially those with influence in governance and regulations, can take action to prevent harmful practices in the design and use of AI. 


{: .amaranth-deep-purple}
### October 2025 - Invited Talk 

I presented a one-hour talk titled **‚ÄúFrom Biased Systems to Fair Practices‚Äù** at the "Diverse and Inclusive Digital Government Working Group" meeting. This Working Group serves as a platform for teams across the Government of Canada to share best practices and lessons learned on initiatives that support recruitment and career mobility for equity-seeking groups, and to promote a more diverse and inclusive digital government.

In this talk, my goal was to shed light on the nature and implications of bias in AI-based systems. I started by explaining why bias has persisted throughout the evolution of AI over the past decade. I then discussed the main sources of bias and the harms they create in real-world applications. I concluded with a discussion on how we can strive to build fair governance practices when deploying biased AI models.



{: .amaranth-deep-purple}
### July 2025 - I was at ACL2025 in Vienna. 

- üèÜ **OUTSTANDING PAPER AWARD:** I am proud to have contributed to creation of PARME, the first machine translation parallel corpora covering eight endangered Middle Eastern languages. This paper was awarded as an oustanding paper at ACL, main conference. Read about PARME [here](https://aclanthology.org/2025.acl-long.1451/).

- We presented a paper at LLMSec workshop on safety risks of fine-tuned LLMs. We show that finetuning, even with benign datasets, reduces safety alignment. This paper is part of a project we did at NRC on behalf of the Canadian AI Safety Institute. Read the paper [here](https://arxiv.org/abs/2506.17209).

{: .amaranth-deep-purple}
### July 2025 - I was at ICML2025 in Vancouver. 
- The report of the 3rd joint AI testing exercise, focused on LLM agents and conducted by the network of international AI Safety Institutes was published at the sidelines of ICML. This testing effort was led by the Singapore AI Safety Institute and some of us from NRC participated in the exercise on behalf of the Canadian AI Safety Institute. I was involved with evluation of the safety of LLM Agents in Farsi. Read the [Blog Post](https://sgaisi.sg/wp-api/wp-content/uploads/2025/07/International-Joint-Testing-Exercise_3JT-Blogpost.pdf) and the [Detailed Report](https://sgaisi.sg/wp-api/wp-content/uploads/2025/07/International-Joint-Testing-Exercise_3JT-Eval-Report-v2.pdf).
  
- I presented at a paper on taxonomy of natural language expalnations at the Workshop on Technical AI Governance. Read our paper [Here](https://arxiv.org/pdf/2507.10585)

{: .amaranth-deep-purple}
### April 2025 - I gave a talk at the NLP course at Northeastern University. Thanks to amazing [Amir Tahmasebi](https://www.linkedin.com/in/amirtahmasebi/) for inviting me to his class! And thanks to students for the great questions and discussions! 

Title: Natural Language Explanations in the XAI Landscape
 
Abstract: In this talk, I first quickly reviewed the main concepts of Explainable AI (XAI) and then focused on natural language explanations. With the rapid advancements in development and adoption of large language models, it is now possible to generate verbalized explanations across different aspects of AI-based decision-making tailored for various goals and audiences. However, research shows that, if not designed thoughtfully, such explanations might be ineffective or even misleading. I explored the critical components of effective explainer modules, discussed the essential characteristics they should embody, and highlighted key evaluation criteria to ensure their reliability and impact.


{: .amaranth-deep-purple}
### Febraury 2025 - Prior to the [AI Action Summit](https://www.elysee.fr/en/sommet-pour-l-action-sur-l-ia) in Paris, we participated in joint multilingual testing of Large Language Models in colaboration with AISIs (AI Safety Institutes) led by Singapore's AISI. 

NRC represented CAISI (Canadian AI Safety Institute) in this endeavour by testing LLMs in three languages, Cantonees, Farsi and Telugu. I was responsible for the Farsi evaluation. Read the full report [here](https://sgaisi.sg/wp-api/wp-content/uploads/2025/06/Improving-Methodologies-for-LLM-Evaluations-Across-Global-Languages-Evaluation-Report-1.pdf). 

{: .amaranth-deep-purple}
### January 2025 - I am honoured to have received the "Digital Technologies' Value for Canada" award at the annual award ceremony at the Digital Technology Research Center, National Research Council Canada. 

I am thrilled that my work on Canada's priorities‚Äî responsible AI for social impact, wildfire management support, and immigration settlement‚Äîhas been recognized by my colleagues and managers. I am extremely grateful for the support and collaboration I received from my colleagues, the management at NRC, and my external partners, who made it possible for me to contribute to these topics that I care about so deeply.

{: .amaranth-deep-purple}
### December 2024 - I was on a panel at the [CCO Learning Day](https://www.canada.ca/en/treasury-board-secretariat/topics/government-communications/communications-community-office/learning-days.html) in Westine Hotel Ottawa. 

I was on a panel on **AI and Trust** at the Communications Community Office (CCO) conference. James Stott, Assistant Secretary, SCMA, TBS, facilitated the discussion and other panellists were Dominic Rochon, Deputy Minister at TBS and Chief Information Officer of Canada, Laurent Charlin, Core Academic member of the Mila Institute, CIFAR co-chair and professor at HEC, Roland-Yves Carignan, Professor of Journalism and Digital Media at UQAM, and Dan Lowcay, Generative AI Innovation Lead for the NRC. The CCO Learning Day is a highly anticipated and well-attended conference that provides training for communicators in the public sector. Speakers include representatives from the federal government, as well as from the academic and private sectors. The event was held on December 10, 2024, at The Westin Hotel in downtown Ottawa. About 1,900 people took part in the event, either in person or online.

{: .amaranth-deep-purple}
### December 2024 - Listen to my podcast on Spotify!

I spoke with amazing [Tobias Stapf](https://www.linkedin.com/in/tobias-stapf-8b604b8/?original_referer=https%3A%2F%2Fwww%2Egoogle%2Ecom%2F&originalSubdomain=de) about my work on how AI and innovation can transform the newcomer experience while prioritizing equity and accessibility in a new "Digital Arrival City" podcast episode. 

üéß Click [here](https://lnkd.in/dqKjEjRu) to listen and join the conversation! 

{: .amaranth-deep-purple}
### November 2024 - I gave a talk on ‚ÄúInnovation in Artificial Intelligence‚Äù at the Canada School of Public Service. 

This talk was part of the "Executive Leadership Development Program‚ÄîModule 4: Leading a Modern Government" and was led by distinguished fellows [Zaina Sovani](https://www.linkedin.com/in/zaina-sovani-2552176/?originalSubdomain=ca) and [Pablo Sobrino](https://pdweek.ca/wp-content/uploads/sites/2/2021/11/Speed-Mentoring-Bio-Pablo-Sobrino-2021-Final.pdf). I talked about two examples of my work in AI for social impact, and [Elise Legendre](https://www.linkedin.com/in/%C3%A9lise-legendre-83645810/?originalSubdomain=ca) from Agriculture and AgriFood Canada talked about her impressive work in leading the adoption of AI in her organization. In Q&A we delved into the use of data and artificial intelligence as innovative tools while highlighting success stories and specific use cases.



{: .amaranth-deep-purple}
### November 2024 - I was on the panel "Citizenship and Belonging in a Globalized and Digitized World" at the [27th Canadian Ethnic Studies Association Biennial Conference](https://cesa-scee.ca/2024-conference/) in Edmonton, Alberta. 

My role was a discussant, and I learned a lot from eight presenters who talked about their fascinating works on how technology has shaped our ever-evolving sense of citizenship and identity. In the context of these presentations, I shared my insights on how social scientists and computer scientists can work together to understand the role of generative AI on belonging and national identity. The panel was chaired by Dr. [Anna Triandafyllidou](https://www.torontomu.ca/bridging-divides/people/directors/anna-triandafyllidou/) 

{: .amaranth-deep-purple}
### November 2024 - We presented the following two papers at the [EMNLP2024](https://2024.emnlp.org/). 

The first paper is on evaluating eight large language models in moral reasoning. This is joint work with Rongchen Guo, Hillary Dawkins, Kathleen C. Fraser, and Svetlana Kiritchenko, and looks at how LLMs, despite their built-in safeguards, can be easily misused to justify harmful language. We also highlighted that they might be used to understand the roots of harmful beliefs to design well-informed interventions. Check out the full paper [Here](https://lnkd.in/eNeN_J7u).

The second paper is on Gender Resolution in Speaker-Listener Dialogue Roles and is joint work with Hillary Dawkins and Jackie Lo. Check out this paper at [Here](https://aclanthology.org/2024.wmt-1.25/).

{: .amaranth-deep-purple}
### November 2024 - Our work on the role of language technologies in immigration settlement has been featured in the Knowledge Mobilization for Settlement Series [Here](https://km4s.ca/publication/social-and-ethical-risks-posed-by-general-purpose-llms-for-settling-newcomers-in-canada-2024/) and [Here](https://km4s.ca/publication/human-centered-ai-applications-for-canadas-immigration-settlement-sector-2024/) and also in [The McGill Daily](https://www.mcgilldaily.com/2024/11/bridging-culture-with-code/).

Many thanks to Marco Campana and Raihana Kamal for spreading the word!


{: .amaranth-deep-purple}
### October 2024 - I attended the [7th AAAI/ACM Conference on AI, Ethics, and Society](https://www.aies-conference.com/2024/) in San Jose, California. 

I presented our work on [Human-Centered AI Applications for Canada‚Äôs Immigration Settlement Sector](https://ojs.aaai.org/index.php/AIES/article/view/31701). I also learned about many fascinating projects taking place at the intersection of AI and social scenes, featured in the conference [Proceedings](https://ojs.aaai.org/index.php/AIES/issue/view/609).



{: .amaranth-deep-purple}
### September 2024 - I was invited to the "Peel-Halton Professional Development and AI webinar Series for Newcomer-Serving Staff and Partner Organizations" to talk about the ‚ÄúRole and Implications of Generative AI and Larger Language Models in Supporting Newcomers.‚Äù 

More than 150 people attended this webinar, and we had a very interesting Q&A session at the end. [Link to the webinar](https://km4s.ca/2024/09/role-of-generative-ai-in-supporting-newcomers-webinar-recording/)


**Abstract:** A successful settlement requires access to accurate information at the right time and a reasonable cost. To enhance the efficiency of information delivery, the settlement sector can significantly benefit from AI-enabled tools. However, while AI research has previously focused on screening and selection processes in immigration, the potential applications of AI in the settlement sector remain understudied. In this talk, I will first explore various applications of AI-enabled language technologies that could be developed for the settlement sector and integrated into its existing service structures. Then, given the foundational role of large language models (LLMs) in developing language technologies, I will caution against the ad-hoc use of general-purpose LLMs and present examples of biases, hallucinations, and functional disparities that could negatively impact newcomers. The talk will conclude with recommendations for creating LLM-based tools specifically designed for the settlement sector and ensuring they are empowering, inclusive, and safe.


{: .amaranth-deep-purple}
### June 2024 - We organized a tutorial on [Risks of General-Purpose LLMs for Settling Newcomers in Canada](https://facctconference.org/2024/acceptedtutorials) at the ACM Fairness, Accountability and Transparency (FAccT) 2024 conference. [Read the Report](https://isarnejad.github.io/files/AI-in-Settlement.pdf){:target="_blank"}

I co-organized this tutorial with [Maryam Molamohammadi](https://mila.quebec/fr/maryam-molamohammadi) and [Samir Bakhtawar](https://www.linkedin.com/in/samirbakht/?originalSubdomain=ca). Our intern, [Kosar Hemmati](https://www.linkedin.com/in/kosar-hemmati/?originalSubdomain=ca), has also put enormous efforts into supporting us for this tutorial. 

**Abstract:** The non-profit settlement sector in Canada supports newcomers in achieving successful integration. This sector faces increasing operational pressures amidst rising immigration targets, which highlights a need for enhanced efficiency and innovation, potentially through reliable AI solutions. The ad-hoc use of general-purpose generative AI, such as ChatGPT, might become a common practice among newcomers and service providers to address this need. However, these tools are not tailored for the settlement domain and can have detrimental implications for immigrants and refugees. We explore the risks that these tools might pose on newcomers to first, warn against the unguarded use of generative AI, and second, to incentivize further research and development in creating AI literacy programs as well as customized LLMs that are aligned with the preferences of the impacted communities. Crucially, such technologies should be designed to integrate seamlessly into the existing workflow of the settlement sector, ensuring human oversight, trustworthiness, and accountability.

---

{: .amaranth-deep-purple}
### February 2024  - I was an invited participant of [Frontiers of Science: Artificial Intelligence](https://rsc-src.ca/en/events/frontiers-science-2024) held in the gorgeous Chateau Laurier in Ottawa. 

The Royal Society of Canada organized this event, and it was a fantastic multidisciplinary meeting focused on early- to mid-career researchers of artificial intelligence. The meeting brought together 20 researchers from Canada and 20 from the UK, with diverse backgrounds in computer science, history, communication, ethics and policy making. The flexible structure of the meeting gave us the opportunity to listen to eye-opening talks from pioneer thinkers of the field and the chance to discuss these ideas in the context of our work in group discussions. For me, the main takeaway was that we will not succeed in achieving much-needed regulations and safeguards around AI unless we break the vicious cycle of fast and profit-oriented science. A safe and reliable AI that benefits all is hard to achieve but worth the fight. 

---

{: .amaranth-deep-purple}
### November 2023 - I co-organized a workshop at the [Pathway to Prosperity national conference](https://site.pheedloop.com/event/p2pvvp2023/home) in Montreal. 

**Title of workshop:** Responsible AI in Settlement Services: Challenges, Social Context, and Ethical AI Solutions 

**Description of workshop:** In this workshop, we explored the potential and limitations of AI language technologies in the immigration context. We first explore the current landscape of settlement services that can be facilitated, assessed, or audited by language technologies. Then, we delve into assessing the potential for a responsible adoption of these technologies in the Canadian settlement service sectors. Some examples encompass translation services, structuring data, detecting patterns on an unparalleled scale, automatic reporting, and auditing fairness in decision-making processes. We conclude by discussing the risks and challenges of deploying these technologies, emphasizing the importance of fostering fair and inclusive technologies.

**Workshop chair:**

Anna Jahn, Director of Public Policy and Learning, MILA - Quebec Artificial Intelligence 


**Speakers:**

Isar Nejadgholi, Senior Research Scientist, National Research Council Canada
*Presentation title:* Promises of Language Technologies for Digital Transformation of the Immigration and Settlement Sector

Maryam Mollamohammadi, Responsible AI Advisor, MILA - Quebec Artificial Intelligence Institute
*Presentation title:* Inclusive and Fair Digital Transformation: Responsible Use of Language Technologies in the Immigration Sector

---

{: .amaranth-deep-purple}
### July 2023 - Best Paper Award

Our paper, **Aporophobia: An Overlooked Type of Toxic Language Targeting the Poor**, won the best paper award at the 7th Workshop on Online Abuse and Harms (WOAH). .[[Paper](https://www.svkir.com/papers/Kiritchenko-et-al-aporophobia-WOAH-2023.pdf)] 


---

{: .amaranth-deep-purple}
### July 2023 - We were at ACL2023 in Toronto and presented the following papers: 

- Svetlana Kiritchenko, Georgina Curto, Isar Nejadgholi, and Kathleen C. Fraser. (2023) Aporophobia: An Overlooked Type of Toxic Language Targeting the Poor. In Proceedings of the 7th Workshop on Online Abuse and Harms (WOAH).[[Paper](https://www.svkir.com/papers/Kiritchenko-et-al-aporophobia-WOAH-2023.pdf)] 

- Fraser, K.C., Kiritchenko, S., Nejadgholi, I., Kerkhof, A. (2023) What Makes a Good Counter-Stereotype? Evaluating Strategies for Automated Responses to Stereotypical Text. In Proceedings of the First Workshop on Social Influence in Conversations (SICon).[[paper](https://www.svkir.com/papers/Fraser-et-al-CounterStereotypes-SICon-2023.pdf)]

- Isar Nejadgholi, Svetlana Kiritchenko, Kathleen C. Fraser, and Esma Balkir. (2023) Concept-Based Explanations to Test for False Causal Relationships Learned by Abusive Language Classifiers. In Proceedings of the 7th Workshop on Online Abuse and Harms (WOAH).[[paper](https://arxiv.org/pdf/2307.01900.pdf)]

- Ghanadian, H., Nejadgholi, I., & Osman, H. A. (2023). ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations. 13th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA at ACL 2023).[[paper](https://arxiv.org/pdf/2306.09390.pdf)]

---

{: .amaranth-deep-purple}
### June 2023 - Best Short Paper Award

Our paper, **Diversity is Not a One-Way Street: Pilot Study on Ethical Interventions for Racial Bias in Text-to-Image Systems.**, won the best short paper award at the 14th International Conference on Computational Creativity (ICCC) in Waterloo. [[Paper](https://www.svkir.com/papers/Fraser-et-al-TextImageBias-ICCC-2023.pdf)]
