---
layout: archive
title: "Projects"
permalink: /research/
author_profile: true
---

{: .amaranth-deep-purple}
## AI for Social Good
  - [**Multimodal Social Media Analysis for Disaster Response:**](#MultiModal-Disaster) This project is funded by New Beginning Initiative Program and is in collaboration with Carleton University. 
  - [**Innovative Solutions to Improve the Immigration System in Canada:**](#Immigration) This project is in collaboration with MILA institute and focuses on using AI to make the immigration settlement in Canada more efficient. 
  - [**Detecting and Countering Abusive Language on Social Media:**](#Det-Count-Stereo) This project focuses on detecting abusive language on social media and countering it using NLP models.
  - [**Social Media Analysis for Early Detection of Suicide Ideation:**](#suicide) This project is in collaboration with the University of Ottawa and focuses on developing tools for the early detection of suicidal notes on social media. 
    
{: .amaranth-deep-purple}
## Computational Social Science
  - [**Uncovering and Combating Stereotypes:**](#Stereo-NL) In this project, we use computational linguistics and machine learning techniques to uncover and combat the stereotypical views prominent in society. 
  - [**Political ideology detection on social media:**](#political-ideology) This project is in collaboration with McGill University, where we use network analysis and natural language processing to understand the political ideologies expressed on social media.

{: .amaranth-deep-purple}
## Ethics in AI: 
  - [**Explainability**](#explain) In this project, we use concept-based explanations to analyze a model with respect to its sensitivity to human-understandable concepts. 
  - [**Behavioral Analysis of Generative Models:**](#Bias) In this project, we use different techniques to uncover biases in text classifiers and generative models.
  - [**Ethical Challenges in Abuisve Language Detection**](#JAIR) In this project, we bring ethical and human rights considerations to every stage of developing an NLP system for detecting abusive language in online platforms. 

    
{: .amaranth-deep-purple}
## Machine Learning Applications
  - [**Medical data analysis**](#biomedical)
  - [**Audio and speech processing**](#Speech)
  - [**Facial recognition**](#PhD)
    
<br>

---

# Selected Papers By Research Topic

---

<br>

{: .gray}
### Detecting and Countering Abusive Language on Social Media:
<a id="Det-Count-Stereo"></a>

- Svetlana Kiritchenko, Georgina Curto, Isar Nejadgholi, and Kathleen C. Fraser. (2023) Aporophobia: An Overlooked Type of Toxic Language Targeting the Poor. In Proceedings of the 7th Workshop on Online Abuse and Harms (WOAH), Toronto, ON, Canada, July 2023. [pdf]
- 


{: .gray}
### Social Media Analysis for early detection of Suicide Ideation:
<a id="suicide"></a>

- Ghanadian, H., Nejadgholi, I., & Osman, H. A. (2023). ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations. 13th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA at ACL 2023), Toronto, Canada, July 2023. [Paper](https://arxiv.org/abs/2306.09390)

{: .gray}
### Uncovering and Combating Stereotypes::
<a id="Stereo-NL"></a>

- Kathleen C. Fraser, Svetlana Kiritchenko, and Isar Nejadgholi. (2022). Computational Modelling of Stereotype Content in Text. Frontiers in Artificial Intelligence, April, 2022. [paper]
- Kathleen C. Fraser, Isar Nejadgholi, and Svetlana Kiritchenko (2021). Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model. In Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021), August 2021. [pdf]
- Fraser, K.C., Kiritchenko, S., Nejadgholi, I., Kerkhof, A. (2023) What Makes a Good Counter-Stereotype? Evaluating Strategies for Automated Responses to Stereotypical Text. In Proceedings of the First Workshop on Social Influence in Conversations (SICon), Toronto, ON, Canada, July 2023. [pdf](https://www.svkir.com/papers/Fraser-et-al-CounterStereotypes-SICon-2023.pdf)
- Kathleen C. Fraser, Svetlana Kiritchenko, and Isar Nejadgholi. (2022). Extracting Age-Related Stereotypes from Social Media Texts. In Proceedings of the Language Resources and Evaluation Conference (LREC-2022), Marseille, France, June 2022. [pdf][project webpage]

{: .gray}
### Political ideology detection on social media:
<a id="political-ideology"></a>


{: .gray}
### Explainability:
<a id="explain"></a>

- Isar Nejadgholi, Svetlana Kiritchenko, Kathleen C. Fraser, and Esma Balkir. (2023) Concept-Based Explanations to Test for False Causal Relationships Learned by Abusive Language Classifiers. In Proceedings of the 7th Workshop on Online Abuse and Harms (WOAH), Toronto, ON, Canada, July 2023. [pdf]
- Isar Nejadgholi, Esma Balkir, Kathleen C. Fraser, and Svetlana Kiritchenko. (2022) Towards Procedural Fairness: Uncovering Biases in How a Toxic Language Classifier Uses Sentiment Information. In Proceedings of the Workshop on Analyzing and Interpreting Neural Networks for NLP (BlackboxNLP), Abu Dhabi, United Arab Emirates, Dec. 2022. [pdf][code]
- Isar Nejadgholi, Kathleen C. Fraser, and Svetlana Kiritchenko. (2022). Improving Generalizability in Implicitly Abusive Language Detection with Concept Activation Vectors. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, Dublin, Ireland, May 2022. [pdf][code]
- Esma Balkir, Isar Nejadgholi, Kathleen C. Fraser, and Svetlana Kiritchenko. (2022). Necessity and Sufficiency for Explaining Text Classifiers: A Case Study in Hate Speech Detection. In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), Seattle, WA, USA, July 2022. [pdf][code]
- Esma Balkir, Svetlana Kiritchenko, Isar Nejadgholi, and Kathleen C. Fraser. (2022) Challenges in Applying Explainability Methods to Improve the Fairness of NLP Models. In Proceedings of the Second Workshop on Trustworthy Natural Language Processing (TrustNLP @ NAACL), Seattle, WA, USA, July 2022. [pdf]
  
{: .gray}
### Behavioral Analysis of Generative Models:
<a id="Bias"></a>
- Fraser, K.C., Kiritchenko, S., Nejadgholi, I. (2023) Diversity is Not a One-Way Street: Pilot Study on Ethical Interventions for Racial Bias in Text-to-Image Systems. In Proceedings of the 14th International Conference on Computational Creativity (ICCC), Waterloo, ON, Canada, June 2023. [pdf](https://www.svkir.com/papers/Fraser-et-al-TextImageBias-ICCC-2023.pdf)
- Fraser, K.C., Nejadgholi, I., Kiritchenko, S. (2023) A Friendly Face: Do Text-to-Image Systems Rely on Stereotypes when the Input is Under-Specified? In Proceedings of the Creative AI Across Modalities Workshop (CreativeAI @ AAAI), Washington, DC, USA, Feb. 2023. [pdf](https://www.svkir.com/papers/Fraser-et-al-CreativeAI-2023.pdf)

{: .gray}
### Ethical Challenges in Abuisve Language Detection:
<a id="JAIR"></a>
- Svetlana Kiritchenko and Isar Nejadgholi. Towards Ethics by Design in Online Abusive Content Detection. NRC Technical Report, October 2020. [pdf]
- Svetlana Kiritchenko, Isar Nejadgholi, and Kathleen C. Fraser. Confronting Abusive Language Online: A Survey from the Ethical and Human Rights Perspective. Journal of Artificial Intelligence Research, 71: 431-478, July 2021. [pdf]
- Isar Nejadgholi and Svetlana Kiritchenko. On Cross-Dataset Generalization in Automatic Detection of Online Abuse. In Proceedings of the 4th Workshop on Online Abuse and Harms at EMNLP-2020, November 2020. [pdf]
  
{: .gray}
### Medical data analysis:
<a id="biomedical"></a>

{: .gray}
### Audio and speech processing:
<a id="Speech"></a>

{: .gray}
### Facial recognition:
<a id="PhD"></a>


